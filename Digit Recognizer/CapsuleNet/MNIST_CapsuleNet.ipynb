{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Image Classification : Implementation in Capsule Network - MNIST Datasets\n",
    "Published Date: 15 October, 2018\n",
    "\n",
    "author: Mohammed Innat\n",
    "email:  innat1994@gmail.com\n",
    "website: https://iphton.github.io/iphton.github.io/\n",
    "\n",
    "\n",
    "Please feel free to use and modify this, but keep the above information. Thanks!\n",
    "\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "# MNIST Implementation Using Capsule Network\n",
    "\n",
    "**CapsNets** are a new neural net architecture that may well have a profound impact on deep learning, in particular for computer vision. **Geoffrey Hinton** proposed a architecture that introduced a completely new type of neural network based on so-called **capsules**. \n",
    "\n",
    "Geoffrey Hinton - [Paper: Dynamic Routing Between Capsules](https://arxiv.org/abs/1710.09829). I found [this](https://www.oreilly.com/ideas/introducing-capsule-networks) blog post by **Aurélien Géron** well explained on this topic. However, most of the implemented function to build **CapsuleNet** is adopted from [Xifeng Guo Ph.D.](https://github.com/XifengGuo).\n",
    "\n",
    "---\n",
    "\n",
    "**About Data:**\n",
    "\n",
    "The data files `train.csv` and `test.csv` contain gray-scale images of hand-drawn digits, from zero through nine.\n",
    "\n",
    "Each image is `28` pixels in height and `28` pixels in width, for a total of `784` pixels in total. Each pixel has a single pixel-value associated with it, indicating the lightness or darkness of that pixel, with higher numbers meaning darker. This pixel-value is an integer between 0 and 255, inclusive.\n",
    "\n",
    "The training data set, (**train.csv**), has `785` columns. The first column, called `label`, is the digit that was drawn by the user. The rest of the columns contain the pixel-values of the associated image.\n",
    "\n",
    "The test data set, (**test.csv**), is the same as the training set, except that it does not contain the `label` column.\n",
    "\n",
    "Find On Kaggle: [Digit Recognizer](https://www.kaggle.com/c/digit-recognizer/data)\n",
    "\n",
    "---\n",
    "\n",
    "**Procedure:**\n",
    "- **2. Data Preprocessing**\n",
    "- **3. Building a Model**\n",
    "- **5. Evaluate the Model**\n",
    "\n",
    "**Miscellaneous:**\n",
    "- Save Model and Weight \n",
    "- Tensorboard: Visualize the Computational Graph and Parameters.\n",
    "\n",
    "Finally, Create `csv` file for kaggle submission.\n",
    "\n",
    "---\n",
    "\n",
    "**Result:**\n",
    "\n",
    "I got **Final accuracy: 0.99** by implementing **Capsule Network** on **GeForce GTX 1050 Ti**. I set 10 epochs on the training process and took 40 mins. However, Training on a single CPU, epochs size should be set within < 3.\n",
    "\n",
    "---\n",
    "\n",
    "Let's begin with importing packages and dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\",category=FutureWarning)\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import callbacks\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras import initializers, layers\n",
    "import tensorflow as tf\n",
    "\n",
    "import keras.backend as K\n",
    "\n",
    "class Length(layers.Layer):\n",
    "    \"\"\"\n",
    "    Compute the length of vectors. This is used to compute a Tensor that has the same shape with y_true in margin_loss\n",
    "    inputs: shape=[dim_1, ..., dim_{n-1}, dim_n]\n",
    "    output: shape=[dim_1, ..., dim_{n-1}]\n",
    "    \"\"\"\n",
    "    def call(self, inputs, **kwargs):\n",
    "        return K.sqrt(K.sum(K.square(inputs), -1))\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[:-1]\n",
    "\n",
    "class Mask(layers.Layer):\n",
    "    \"\"\"\n",
    "    Mask a Tensor with shape=[None, d1, d2] by the max value in axis=1.\n",
    "    Output shape: [None, d2]\n",
    "    \"\"\"\n",
    "    def call(self, inputs, **kwargs):\n",
    "        # use true label to select target capsule, shape=[batch_size, num_capsule]\n",
    "        if type(inputs) is list:  # true label is provided with shape = [batch_size, n_classes], i.e. one-hot code.\n",
    "            assert len(inputs) == 2\n",
    "            inputs, mask = inputs\n",
    "        else:  # if no true label, mask by the max length of vectors of capsules\n",
    "            x = inputs\n",
    "            # Enlarge the range of values in x to make max(new_x)=1 and others < 0\n",
    "            x = (x - K.max(x, 1, True)) / K.epsilon() + 1\n",
    "            mask = K.clip(x, 0, 1)  # the max value in x clipped to 1 and other to 0\n",
    "\n",
    "        # masked inputs, shape = [batch_size, dim_vector]\n",
    "        inputs_masked = K.batch_dot(inputs, mask, [1, 1])\n",
    "        return inputs_masked\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        if type(input_shape[0]) is tuple:  # true label provided\n",
    "            return tuple([None, input_shape[0][-1]])\n",
    "        else:\n",
    "            return tuple([None, input_shape[-1]])\n",
    "\n",
    "\n",
    "def squash(vectors, axis=-1):\n",
    "    \"\"\"\n",
    "    The non-linear activation used in Capsule. It drives the length of a large vector to near 1 and small vector to 0\n",
    "    :param vectors: some vectors to be squashed, N-dim tensor\n",
    "    :param axis: the axis to squash\n",
    "    :return: a Tensor with same shape as input vectors\n",
    "    \"\"\"\n",
    "    s_squared_norm = K.sum(K.square(vectors), axis, keepdims=True)\n",
    "    scale = s_squared_norm / (1 + s_squared_norm) / K.sqrt(s_squared_norm)\n",
    "    return scale * vectors\n",
    "\n",
    "\n",
    "class CapsuleLayer(layers.Layer):\n",
    "    \"\"\"\n",
    "    The capsule layer. It is similar to Dense layer. Dense layer has `in_num` inputs, each is a scalar, the output of the \n",
    "    neuron from the former layer, and it has `out_num` output neurons. CapsuleLayer just expand the output of the neuron\n",
    "    from scalar to vector. So its input shape = [None, input_num_capsule, input_dim_vector] and output shape = \\\n",
    "    [None, num_capsule, dim_vector]. For Dense Layer, input_dim_vector = dim_vector = 1.\n",
    "    \n",
    "    :param num_capsule: number of capsules in this layer\n",
    "    :param dim_vector: dimension of the output vectors of the capsules in this layer\n",
    "    :param num_routings: number of iterations for the routing algorithm\n",
    "    \"\"\"\n",
    "    def __init__(self, num_capsule, dim_vector, num_routing=3,\n",
    "                 kernel_initializer='glorot_uniform',\n",
    "                 bias_initializer='zeros',\n",
    "                 **kwargs):\n",
    "        super(CapsuleLayer, self).__init__(**kwargs)\n",
    "        self.num_capsule = num_capsule\n",
    "        self.dim_vector = dim_vector\n",
    "        self.num_routing = num_routing\n",
    "        self.kernel_initializer = initializers.get(kernel_initializer)\n",
    "        self.bias_initializer = initializers.get(bias_initializer)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) >= 3, \"The input Tensor should have shape=[None, input_num_capsule, input_dim_vector]\"\n",
    "        self.input_num_capsule = input_shape[1]\n",
    "        self.input_dim_vector = input_shape[2]\n",
    "\n",
    "        # Transform matrix\n",
    "        self.W = self.add_weight(shape=[self.input_num_capsule, self.num_capsule, self.input_dim_vector, self.dim_vector],\n",
    "                                 initializer=self.kernel_initializer,\n",
    "                                 name='W')\n",
    "\n",
    "        # Coupling coefficient. The redundant dimensions are just to facilitate subsequent matrix calculation.\n",
    "        self.bias = self.add_weight(shape=[1, self.input_num_capsule, self.num_capsule, 1, 1],\n",
    "                                    initializer=self.bias_initializer,\n",
    "                                    name='bias',\n",
    "                                    trainable=False)\n",
    "        self.built = True\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        # inputs.shape=[None, input_num_capsule, input_dim_vector]\n",
    "        # Expand dims to [None, input_num_capsule, 1, 1, input_dim_vector]\n",
    "        inputs_expand = K.expand_dims(K.expand_dims(inputs, 2), 2)\n",
    "\n",
    "        # Replicate num_capsule dimension to prepare being multiplied by W\n",
    "        # Now it has shape = [None, input_num_capsule, num_capsule, 1, input_dim_vector]\n",
    "        inputs_tiled = K.tile(inputs_expand, [1, 1, self.num_capsule, 1, 1])\n",
    "\n",
    "     \n",
    "        # Compute `inputs * W` by scanning inputs_tiled on dimension 0. This is faster but requires Tensorflow.\n",
    "        # inputs_hat.shape = [None, input_num_capsule, num_capsule, 1, dim_vector]\n",
    "        inputs_hat = tf.scan(lambda ac, x: K.batch_dot(x, self.W, [3, 2]),\n",
    "                             elems=inputs_tiled,\n",
    "                             initializer=K.zeros([self.input_num_capsule, self.num_capsule, 1, self.dim_vector]))\n",
    "        \n",
    "        # Routing algorithm V2. Use iteration. V2 and V1 both work without much difference on performance\n",
    "        assert self.num_routing > 0, 'The num_routing should be > 0.'\n",
    "        for i in range(self.num_routing):\n",
    "            c = tf.nn.softmax(self.bias, dim=2)  # dim=2 is the num_capsule dimension\n",
    "            # outputs.shape=[None, 1, num_capsule, 1, dim_vector]\n",
    "            outputs = squash(K.sum(c * inputs_hat, 1, keepdims=True))\n",
    "\n",
    "            # last iteration needs not compute bias which will not be passed to the graph any more anyway.\n",
    "            if i != self.num_routing - 1:\n",
    "                # self.bias = K.update_add(self.bias, K.sum(inputs_hat * outputs, [0, -1], keepdims=True))\n",
    "                self.bias += K.sum(inputs_hat * outputs, -1, keepdims=True)\n",
    "            # tf.summary.histogram('BigBee', self.bias)  # for debugging\n",
    "        return K.reshape(outputs, [-1, self.num_capsule, self.dim_vector])\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return tuple([None, self.num_capsule, self.dim_vector])\n",
    "\n",
    "\n",
    "def PrimaryCap(inputs, dim_vector, n_channels, kernel_size, strides, padding):\n",
    "    \"\"\"\n",
    "    Apply Conv2D `n_channels` times and concatenate all capsules\n",
    "    :param inputs: 4D tensor, shape=[None, width, height, channels]\n",
    "    :param dim_vector: the dim of the output vector of capsule\n",
    "    :param n_channels: the number of types of capsules\n",
    "    :return: output tensor, shape=[None, num_capsule, dim_vector]\n",
    "    \"\"\"\n",
    "    output = layers.Conv2D(filters=dim_vector*n_channels, kernel_size=kernel_size, strides=strides, padding=padding)(inputs)\n",
    "    outputs = layers.Reshape(target_shape=[-1, dim_vector])(output)\n",
    "    return layers.Lambda(squash)(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-1-3bff8318abf8>:121: calling softmax (from tensorflow.python.ops.nn_ops) with dim is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "dim is deprecated, use axis instead\n",
      "WARNING:tensorflow:Variable += will be deprecated. Use variable.assign_add if you want assignment to the variable value or 'x = x + y' if you want a new python Tensor object.\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 28, 28, 1)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1 (Conv2D)                  (None, 20, 20, 256)  20992       input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 6, 6, 256)    5308672     conv1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)             (None, 1152, 8)      0           conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 1152, 8)      0           reshape_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "digitcaps (CapsuleLayer)        (None, 10, 16)       1486080     lambda_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 10)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "mask_1 (Mask)                   (None, 16)           0           digitcaps[0][0]                  \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 512)          8704        mask_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 1024)         525312      dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 784)          803600      dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "out_caps (Length)               (None, 10)           0           digitcaps[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "out_recon (Reshape)             (None, 28, 28, 1)    0           dense_3[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 8,153,360\n",
      "Trainable params: 8,141,840\n",
      "Non-trainable params: 11,520\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras import layers, models\n",
    "from keras import backend as K\n",
    "from keras.utils import to_categorical\n",
    "import keras\n",
    "\n",
    "def CapsNet(input_shape, n_class, num_routing):\n",
    "    \"\"\"\n",
    "    A Capsule Network on MNIST.\n",
    "    :param input_shape: data shape, 4d, [None, width, height, channels]\n",
    "    :param n_class: number of classes\n",
    "    :param num_routing: number of routing iterations\n",
    "    :return: A Keras Model with 2 inputs and 2 outputs\n",
    "    \"\"\"\n",
    "    x = layers.Input(shape=input_shape)\n",
    "\n",
    "    # Layer 1: Just a conventional Conv2D layer\n",
    "    conv1 = layers.Conv2D(filters=256, kernel_size=9, strides=1, padding='valid', activation='relu', name='conv1')(x)\n",
    "\n",
    "    # Layer 2: Conv2D layer with `squash` activation, then reshape to [None, num_capsule, dim_vector]\n",
    "    primarycaps = PrimaryCap(conv1, dim_vector=8, n_channels=32, kernel_size=9, strides=2, padding='valid')\n",
    "\n",
    "    # Layer 3: Capsule layer. Routing algorithm works here.\n",
    "    digitcaps = CapsuleLayer(num_capsule=n_class, dim_vector=16, num_routing=num_routing, name='digitcaps')(primarycaps)\n",
    "\n",
    "    # Layer 4: This is an auxiliary layer to replace each capsule with its length. Just to match the true label's shape.\n",
    "    # If using tensorflow, this will not be necessary. :)\n",
    "    out_caps = Length(name='out_caps')(digitcaps)\n",
    "\n",
    "    # Decoder network.\n",
    "    y = layers.Input(shape=(n_class,))\n",
    "    masked = Mask()([digitcaps, y])  # The true label is used to mask the output of capsule layer.\n",
    "    x_recon = layers.Dense(512, activation='relu')(masked)\n",
    "    x_recon = layers.Dense(1024, activation='relu')(x_recon)\n",
    "    x_recon = layers.Dense(784, activation='sigmoid')(x_recon)\n",
    "    x_recon = layers.Reshape(target_shape=[28, 28, 1], name='out_recon')(x_recon)\n",
    "\n",
    "    # two-input-two-output keras Model\n",
    "    return models.Model([x, y], [out_caps, x_recon])\n",
    "\n",
    "def margin_loss(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Margin loss for Eq.(4). When y_true[i, :] contains not just one `1`, this loss should work too. Not test it.\n",
    "    :param y_true: [None, n_classes]\n",
    "    :param y_pred: [None, num_capsule]\n",
    "    :return: a scalar loss value.\n",
    "    \"\"\"\n",
    "    L = y_true * K.square(K.maximum(0., 0.9 - y_pred)) + \\\n",
    "        0.5 * (1 - y_true) * K.square(K.maximum(0., y_pred - 0.1))\n",
    "\n",
    "    return K.mean(K.sum(L, 1))\n",
    "\n",
    "# define model\n",
    "model = CapsNet(input_shape=[28, 28, 1],\n",
    "                n_class=10,\n",
    "                num_routing=3)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Digit Counter:\n",
      " 1    4684\n",
      "7    4401\n",
      "3    4351\n",
      "9    4188\n",
      "2    4177\n",
      "6    4137\n",
      "0    4132\n",
      "4    4072\n",
      "8    4063\n",
      "5    3795\n",
      "Name: label, dtype: int64\n",
      "------------------------------\n",
      "Check for Missing values:\n",
      " count       784\n",
      "unique        1\n",
      "top       False\n",
      "freq        784\n",
      "dtype: object \n",
      "\n",
      "------------------------------\n",
      "Check for Missing values:\n",
      " count       784\n",
      "unique        1\n",
      "top       False\n",
      "freq        784\n",
      "dtype: object \n",
      "\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train = pd.read_csv(\"Data/train.csv\")\n",
    "test = pd.read_csv(\"Data/test.csv\")\n",
    "\n",
    "Y_train = train[\"label\"]\n",
    "\n",
    "# Drop 'label' column\n",
    "X_train = train.drop(labels = [\"label\"],axis = 1) \n",
    "\n",
    "\n",
    "print('Digit Counter:\\n',Y_train.value_counts())\n",
    "print('-'*30)\n",
    "print('Check for Missing values:\\n',X_train.isnull().any().describe() , '\\n')\n",
    "print('-'*30)\n",
    "print('Check for Missing values:\\n',test.isnull().any().describe() , '\\n')\n",
    "print('-'*30)\n",
    "\n",
    "# Normalize the data\n",
    "X_train = X_train / 255.0\n",
    "\n",
    "# Reshape image in 3 dimensions \n",
    "X_train = X_train.values.reshape(-1,28,28,1)\n",
    "test = test.values.reshape(-1, 28, 28, 1).astype('float32') / 255.\n",
    "\n",
    "from keras.utils.np_utils import to_categorical \n",
    "Y_train = to_categorical(Y_train, num_classes = 10)\n",
    "\n",
    "\n",
    "# Set the random seed\n",
    "random_seed = 101\n",
    "\n",
    "# Randomly split the data sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Split the train and the validation set for the fitting\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size = 0.1,\n",
    "                                                  random_state=random_seed)\n",
    "\n",
    "\n",
    "def train(model, data, epoch_size_frac=1.0):\n",
    "    \"\"\"\n",
    "    Training a CapsuleNet\n",
    "    :param model: the CapsuleNet model\n",
    "    :param data: a tuple containing training and testing data, like `((x_train, y_train), (x_test, y_test))`\n",
    "    :param args: arguments\n",
    "    :return: The trained model\n",
    "    \"\"\"\n",
    "    # unpacking the data\n",
    "    (x_train, y_train), (x_val, y_val) = data\n",
    "\n",
    "    # callbacks\n",
    "    log = callbacks.CSVLogger('CapsuleNet/log.csv')\n",
    "    checkpoint = callbacks.ModelCheckpoint('CapsuleNet/CapsuleNet Weights Each Epochs/weights-{epoch:02d}.h5',\n",
    "                                           save_best_only=True, save_weights_only=True, verbose=1)\n",
    "    lr_decay = callbacks.LearningRateScheduler(schedule = lambda epoch: 0.001 * np.exp(-epoch / 10.))\n",
    "\n",
    "    # compile the model\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss=[margin_loss, 'mse'],\n",
    "                  loss_weights=[1., 0.0005],\n",
    "                  metrics={'out_caps': 'accuracy'})\n",
    "    \n",
    "    # Using the Tensorboard callback of Keras. \n",
    "    # https://stackoverflow.com/questions/42112260/how-do-i-use-the-tensorboard-callback-of-keras\n",
    "    tbCallBack = keras.callbacks.TensorBoard(log_dir='CapsuleNet/CapGraph', histogram_freq=0, \n",
    "                                         write_graph=True, write_images=True)\n",
    "\n",
    "\n",
    "    # -----------------------------------Begin: Training with data augmentation -----------------------------------#\n",
    "    def train_generator(x, y, batch_size, shift_fraction=0.):\n",
    "        train_datagen = ImageDataGenerator(width_shift_range = shift_fraction,\n",
    "                                           height_shift_range = shift_fraction)  # shift up to 2 pixel for MNIST\n",
    "        generator = train_datagen.flow(x, y, batch_size=batch_size)\n",
    "        while 1:\n",
    "            x_batch, y_batch = generator.next()\n",
    "            yield ([x_batch, y_batch], [y_batch, x_batch])\n",
    "\n",
    "    # Training with data augmentation. \n",
    "    model.fit_generator(generator=train_generator(x_train, y_train, 64, 0.1),\n",
    "                        steps_per_epoch=int(epoch_size_frac*y_train.shape[0] / 64),\n",
    "                        epochs = 10,\n",
    "                        validation_data = [[x_val, y_val], [y_val, x_val]],\n",
    "                        callbacks = [log, checkpoint, lr_decay, tbCallBack])\n",
    "    # -----------------------------------End: Training with data augmentation -----------------------------------#\n",
    "\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "295/295 [==============================] - 240s 813ms/step - loss: 0.1776 - out_caps_loss: 0.1775 - out_recon_loss: 0.1660 - out_caps_acc: 0.8103 - val_loss: 0.0395 - val_out_caps_loss: 0.0395 - val_out_recon_loss: 0.0703 - val_out_caps_acc: 0.9752\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.03953, saving model to CapsuleNet/weights-01.h5\n",
      "Epoch 2/10\n",
      "295/295 [==============================] - 238s 807ms/step - loss: 0.0480 - out_caps_loss: 0.0480 - out_recon_loss: 0.0617 - out_caps_acc: 0.9668 - val_loss: 0.0314 - val_out_caps_loss: 0.0313 - val_out_recon_loss: 0.0677 - val_out_caps_acc: 0.9848\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.03953 to 0.03136, saving model to CapsuleNet/weights-02.h5\n",
      "Epoch 3/10\n",
      "295/295 [==============================] - 239s 810ms/step - loss: 0.0338 - out_caps_loss: 0.0338 - out_recon_loss: 0.0608 - out_caps_acc: 0.9771 - val_loss: 0.0256 - val_out_caps_loss: 0.0256 - val_out_recon_loss: 0.0670 - val_out_caps_acc: 0.9860\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.03136 to 0.02564, saving model to CapsuleNet/weights-03.h5\n",
      "Epoch 4/10\n",
      "295/295 [==============================] - 238s 808ms/step - loss: 0.0303 - out_caps_loss: 0.0303 - out_recon_loss: 0.0607 - out_caps_acc: 0.9786 - val_loss: 0.0177 - val_out_caps_loss: 0.0177 - val_out_recon_loss: 0.0664 - val_out_caps_acc: 0.9912\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.02564 to 0.01769, saving model to CapsuleNet/weights-04.h5\n",
      "Epoch 5/10\n",
      "295/295 [==============================] - 239s 809ms/step - loss: 0.0237 - out_caps_loss: 0.0237 - out_recon_loss: 0.0603 - out_caps_acc: 0.9840 - val_loss: 0.0226 - val_out_caps_loss: 0.0225 - val_out_recon_loss: 0.0657 - val_out_caps_acc: 0.9869\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.01769\n",
      "Epoch 6/10\n",
      "295/295 [==============================] - 238s 806ms/step - loss: 0.0212 - out_caps_loss: 0.0212 - out_recon_loss: 0.0595 - out_caps_acc: 0.9844 - val_loss: 0.0159 - val_out_caps_loss: 0.0158 - val_out_recon_loss: 0.0651 - val_out_caps_acc: 0.9890\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.01769 to 0.01587, saving model to CapsuleNet/weights-06.h5\n",
      "Epoch 7/10\n",
      "295/295 [==============================] - 238s 807ms/step - loss: 0.0176 - out_caps_loss: 0.0176 - out_recon_loss: 0.0593 - out_caps_acc: 0.9879 - val_loss: 0.0128 - val_out_caps_loss: 0.0128 - val_out_recon_loss: 0.0641 - val_out_caps_acc: 0.9924\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.01587 to 0.01285, saving model to CapsuleNet/weights-07.h5\n",
      "Epoch 8/10\n",
      "295/295 [==============================] - 239s 811ms/step - loss: 0.0176 - out_caps_loss: 0.0175 - out_recon_loss: 0.0588 - out_caps_acc: 0.9868 - val_loss: 0.0121 - val_out_caps_loss: 0.0120 - val_out_recon_loss: 0.0633 - val_out_caps_acc: 0.9912\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.01285 to 0.01208, saving model to CapsuleNet/weights-08.h5\n",
      "Epoch 9/10\n",
      "295/295 [==============================] - 239s 810ms/step - loss: 0.0164 - out_caps_loss: 0.0164 - out_recon_loss: 0.0584 - out_caps_acc: 0.9870 - val_loss: 0.0110 - val_out_caps_loss: 0.0110 - val_out_recon_loss: 0.0624 - val_out_caps_acc: 0.9910\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.01208 to 0.01100, saving model to CapsuleNet/weights-09.h5\n",
      "Epoch 10/10\n",
      "295/295 [==============================] - 239s 811ms/step - loss: 0.0137 - out_caps_loss: 0.0136 - out_recon_loss: 0.0579 - out_caps_acc: 0.9900 - val_loss: 0.0117 - val_out_caps_loss: 0.0116 - val_out_recon_loss: 0.0620 - val_out_caps_acc: 0.9914\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.01100\n",
      "Wall time: 41min 34s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.engine.training.Model at 0x1e82d1eb438>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "train(model=model, data=((X_train, Y_train), (X_val, Y_val)), \n",
    "      epoch_size_frac = 0.5) # do 10% of an epoch (takes too long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tesing....\n",
      "--------------------\n",
      "Test acc: 0.99\n"
     ]
    }
   ],
   "source": [
    "def test(model, data):\n",
    "    x_test, y_test = data\n",
    "    y_pred, x_recon = model.predict([x_test, y_test], batch_size=100)\n",
    "    print('Tesing....')\n",
    "    print('-'*20)\n",
    "    print('Test acc:', np.sum(np.argmax(y_pred, 1) == np.argmax(y_test, 1))/y_test.shape[0])\n",
    "\n",
    "# call test function    \n",
    "test(model = model, data = ( X_val[:100],Y_val[:100] ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving Model and Weights\n",
    "We can save the **model** in `json` and **weights** in a `hdf5` file format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving Model Into JSON.\n",
      "Saving Trained Model Weights Into HDF5.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import model_from_json\n",
    "\n",
    "# serialize model to JSON\n",
    "# the keras model which is trained is defined as 'model' in this example\n",
    "model_json = model.to_json()\n",
    "print('Saving Model Into JSON.')\n",
    "with open(\"CapsuleNet/CapsuleTrained_Model/trained_model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "\n",
    "# previously we saved weight for each epochs. Weight of last epoch matched. \n",
    "# saving weights to HDF5\n",
    "print('Saving Trained Model Weights Into HDF5.')\n",
    "model.save_weights('CapsuleNet/CapsuleTrained_Model/trained_model_weights.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorboard\n",
    "[Run Tensorboard](https://github.com/lspvic/jupyter_tensorboard) from jupyter notebook. To run from terminal: `tensorboard --logdir ./CapGraph`. In my case, I run this command from my working directory where a folder (Graph) has been created."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28000/28000 [==============================] - 111s 4ms/step\n"
     ]
    }
   ],
   "source": [
    "# creating kaggle csv submission file\n",
    "y_pred, _ = model.predict([test, \n",
    "                           np.zeros((data_test.shape[0],10))], \n",
    "                           batch_size = 32, \n",
    "                          verbose = True)    \n",
    "\n",
    "with open('CapsuleNet/submission.csv', 'w') as out_file:\n",
    "    out_file.write('ImageId,Label\\n')\n",
    "    for img_id, guess_label in enumerate(np.argmax(y_pred,1),1):\n",
    "        out_file.write('%d,%d\\n' % (img_id, guess_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
